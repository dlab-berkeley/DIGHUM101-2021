{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 1: spaCy Linguistic Features\n",
    "\n",
    "Use spaCy to create a [linguistic features table](https://spacy.io/usage/linguistic-features). [Part of speech tagging](https://stackabuse.com/python-for-nlp-parts-of-speech-tagging-and-named-entity-recognition/) is a means to \"[assign] parts of speech to individual words in a sentence\". \n",
    "\n",
    "What are the [ten parts of speech](https://www.theclassroom.com/10-parts-speech-8344653.html)?\n",
    "\n",
    "[Click here to view part of speech tagging abbreviations](https://medium.com/@gianpaul.r/tokenization-and-parts-of-speech-pos-tagging-in-pythons-nltk-library-2d30f70af13b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from string import punctuation\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the large pretrained model\n",
    "# (load sm or md if it crashes your computer)\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our function \n",
    "def lemmatize(tokens):\n",
    "    \"\"\"Return the lemmas for each word in `tokens`.\"\"\"\n",
    "    # spacy models operate on strings not lists, so we turn the tokens back into\n",
    "    # a string of words\n",
    "    words = ' '.join(tokens)\n",
    "    # this line does all sorts of processing, including the lemmatization.\n",
    "    # `doc` will be like a list of tokens that we can iterate over\n",
    "    doc = nlp(words)\n",
    "    # each token in `doc` holds information about that token. The `lemma_`\n",
    "    # attribute holds the lemma of that token represented as a string. For\n",
    "    # performance reasons, the `lemma` (without the trailing underscore) holds\n",
    "    # an integer representation of the token, that we'll rarely ever need.\n",
    "    return [token.lemma_ for token in doc]\n",
    "\n",
    "tokens = ('''I was thinking if off the top of your head you are aware of a \n",
    "generalizable comprehension to quickly stem all words in a list of tokens and \n",
    "how to quickly write up a one-minute example? This will be really useful for \n",
    "students interested in text preprocessing.''').split()\n",
    "\n",
    "lemmas = lemmatize(tokens)\n",
    "# Notice that spacy lemmatizes pronouns (e.g. \"you\", \"I\", \"your\") in a funny way '-PRON-'.\n",
    "# It just tells us that they are pronouns, rather than giving us something like\n",
    "# \"your\" -> \"you\".\n",
    "print(lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another example: \n",
    "    \n",
    "Now that our function is defined, let's try it on another sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does this cell work correctly? Does it give us any extraneous information?\n",
    "tokens2 = (\"Thinking, jumping, running, quicking, eating, quickly - all in a days work for me, you, she, and he!\").split()\n",
    "\n",
    "lemmas2 = lemmatize(tokens2)\n",
    "print(lemmas2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation ...\n",
    "sentence = \"Thinking, jumping, running, quicking, eating, quickly - all in a days work for me, you, she, and he!\"\n",
    "\n",
    "for char in punctuation:\n",
    "    sentence = sentence.replace(char, \"\")\n",
    "    \n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-run the lemmatizer!\n",
    "# Why is \"quickly\" not lemmatized? (Because it is an adverb perhaps? Is 'quicking' a word?)\n",
    "tokens3 = sentence.split()\n",
    "\n",
    "lemmas3 = lemmatize(tokens3)\n",
    "print(lemmas3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to data frame\n",
    "\n",
    "Convert the linguistic features table to a data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define doc\n",
    "doc = nlp(\" \".join(tokens3))\n",
    "\n",
    "d = []\n",
    "for token in doc:\n",
    "    d.append((token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "          token.shape_, token.is_alpha, token.is_stop))\n",
    "\n",
    "out = pd.DataFrame(d, columns=(\"text\", \"lemma\", \"pos\", \"tag\", \"dep\", \"shape\", \n",
    "                               \"is_alpha\", \"is_stop\"))\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 2: Repeat\n",
    "\n",
    "Repeat Challenge 1 with a text of your choosing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 3: Context\n",
    "\n",
    "If you are doing anything that involves text for your individual final project, be sure to start thinking about **_why_** you might want to use n-grams, word2vec, or BERT instead of single-word tokenization. Start brainstorming now even if it all doesn't make total yet!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
