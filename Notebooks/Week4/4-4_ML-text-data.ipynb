{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning and text data\n",
    "\n",
    "If we have a corpus of texts, we first must preprocess those texts into a format the algorithms can understand. This usually means converting the representation of our text into numbers!\n",
    "\n",
    "# Bag of words model\n",
    "\n",
    "A bag of words model classifies a text by turning it into a \"bag\" of words to normalize and count them. Scikit-learn's `CountVectorizer` function helps us do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CountVectorizer\n",
    "\n",
    "`CountVectorizer` will help us quickly tokenize text, learn its vocabulary, and encode the text as a vector for use in machine learning. This is often referred to as document encoding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a corpus\n",
    "corpus = [\n",
    "    \"This is the first document.\",\n",
    "    \"This is the second second document.\",\n",
    "    \"And the third one.\",\n",
    "    \"Is this is the first is document and?\"\n",
    "    ]\n",
    "\n",
    "# Define an empty bag (of words)\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Use the .fit method to tokenize the text and learn the vocabulary\n",
    "vectorizer.fit(corpus)\n",
    "\n",
    "# Print the vocabulary\n",
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our output is a dictionary. What are the keys and values? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document term matrix\n",
    "\n",
    "A [document term matrix](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) displays term frequencies that occur across a collection of documents. We want to encode the documents into a [sparse matrix](https://sebastianraschka.com/faq/docs/bag-of-words-sparsity.html#:~:text=By%20definition%2C%20a%20sparse%20matrix,as%20a%20word%2Dcount%20vector.&text=Thus%2C%20if%20most%20of%20your,most%20likely%20sparse%20as%20well!) to represent the frequencies of each vocabular word across the documents.\n",
    "\n",
    "The column headers could read **(document number, vocabulary word)   frequency**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the documents\n",
    "vector = vectorizer.transform(corpus)\n",
    "print(vector) # 4 x 9 sparse matrix - four documents with nine words across them!\n",
    "print(vector.shape)\n",
    "print(type(vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the vector as arrays (4 x 9). Nice!!\n",
    "# Each row is a document\n",
    "# Each column is a vocabulary word (0 thru 8)\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the arrays in the above cell. \n",
    "# In which documents does \"and\" appear? \n",
    "# What about \"document\"? What about \"the\"?\n",
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What does this tell us? \n",
    "vectorizer.transform(['document']).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigrams\n",
    "\n",
    "In addition to uni-grams, using bigrams can be useful to preserve some ordering information. Here we can look at two (bi) or three (tri) or four (quad) or more words at a time! \n",
    "\n",
    "> NOTE: **`ngram_range=(1,2)`** will get you bigrams, **`ngram_range=(1,3)`** will get you tri-grams, **`ngram_range=(1,4)`** will get you quad-grams, etc. \n",
    "\n",
    "> **`token_pattern=r'\\b\\w+\\b'`** is standard regex code to separate words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a bigram bag of words\n",
    "bigram_vectorizer = CountVectorizer(ngram_range = (1,2),\n",
    "                                    token_pattern = r'\\b\\w+\\b', \n",
    "                                    min_df = 1)\n",
    "bigram_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the bigram bag of words\n",
    "analyze = bigram_vectorizer.build_analyzer()\n",
    "analyze('Bi-grams. Are cool!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply this idea to our `corpus` variable from above\n",
    "\n",
    "Why do we have four rows? \n",
    "\n",
    "How many columns do we have? What do they represent? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corpus transformation\n",
    "x = bigram_vectorizer.fit_transform(corpus).toarray()\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are feature names? The column names! The rows are our documents :) \n",
    "bigram_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is going on here? \n",
    "# Search for which document a certain vocabular word appears\n",
    "feature_index = bigram_vectorizer.vocabulary_.get('is the')\n",
    "x[:, feature_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document encoding / machine learning (continued)\n",
    "\n",
    "[Term frequency–inverse document frequency (TFIDF)](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) can be thought of as an extension of `CountVectorizer`. However, instead of counting words, TFIDF identifies unique words within and across documents. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocabulary\n",
    "- **Document Term Matrix:** Is a matrix that describes the frequency of terms that occur in a collection of documents. In a document-term matrix, rows correspond to documents in the collection and columns correspond to terms.\n",
    "\n",
    "- **TF-IDF Scores:** Short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus.\n",
    "\n",
    "- **Topic Modeling:** A general class of statistical models that uncover abstract topics within a text. It uses the co-occurrence of words within documents, compared to their distribution across documents, to uncover these abstract themes. The output is a list of weighted words, which indicate the subject of each topic, and a weight distribution across topics for each document.\n",
    "    \n",
    "- **LDA:** Latent Dirichlet Allocation. A particular model for topic modeling. It does not take document order into account, unlike other topic modeling algorithms. Also see word2vec and BERT! (Week 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DTM/TF-IDF\n",
    "\n",
    "- Let's use Python's scikit-learn package learn to make a document term matrix from the dataset `music_reviews.csv` (collected from [Metacritic](https://www.metacritic.com/)). We will then use the DTM and a word weighting technique called TF-IDF (term frequency inverse document frequency) to identify important and discerning words within this dataset with Pandas.\n",
    "\n",
    "We ask the question: **what words distinguish reviews of Rap albums, Indie Rock albums, and Jazz albums?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_csv(\"../../Data/music_reviews.csv\", sep = \"\\t\")\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review - Explore the Data with Pandas\n",
    "\n",
    "Let's first explore the data. This serves not only as a basic informative purpose, but also to ensure there are not any glaring errors. \n",
    "\n",
    "First, what genres are in this dataset, and how many reviews in each genre?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews['genre'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Who were the artists?\n",
    "reviews.artist.value_counts().head(20)\n",
    "\n",
    "# or\n",
    "\n",
    "# reviews['artist'].value_counts().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Who were the reviewers?\n",
    "reviews['critic'].value_counts().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What was the distribution of review scores like?\n",
    "reviews['score'].plot(kind='hist', \n",
    "                      bins = 50, \n",
    "                      figsize = (6, 3)); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View average score by genre\n",
    "reviews_grouped_by_genre = reviews.groupby(\"genre\")\n",
    "# print(reviews_grouped_by_genre)\n",
    "reviews_grouped_by_genre['score'].mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Together, let's make barplots for the number of reviews by genre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get frequencies (counts) for the number of reviews by genre\n",
    "reviews[\"genre\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert this to a data frame\n",
    "gen = pd.DataFrame(reviews[\"genre\"].value_counts())\n",
    "gen = gen.reset_index()\n",
    "gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out the new column names\n",
    "list(gen.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename these columns\n",
    "gen = gen.rename(columns = {\"index\":\"GENRE\", \"genre\":\"COUNT\"})\n",
    "gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the plot\n",
    "gen_fig = sns.barplot(x = 'COUNT', \n",
    "                      y = 'GENRE', \n",
    "                      data = gen, \n",
    "                      orient = 'h')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also make barplots for average review score by genre and boxplots for the review scores by genre. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_review = reviews.groupby('genre')['score'].mean()\n",
    "mean_review.plot.barh(rot = 15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Boxplots of average score by genre\n",
    "sns.boxplot(x = \"score\", y = \"genre\", data = reviews);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wait, so what about TF-IDF?\n",
    "\n",
    "Now that we have a sense of how these albums were scored by genre we can take a look at the language of the reviews themselves to see how those words might relate to the album scores!\n",
    "\n",
    "> NOTE: remember that exploring your data with basic summary statistics and visualizations is a good first step before anything more complex!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is going on here?\n",
    "def remove_digits(comment):\n",
    "    return ''.join([ch for ch in comment if not ch.isdigit()])\n",
    "\n",
    "reviews['body_without_digits'] = reviews['body'].apply(remove_digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the first body entry\n",
    "list(reviews[\"body\"])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View that same body entry - but without digits! What happened?\n",
    "list(reviews[\"body_without_digits\"])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `CountVectorizer` revisited\n",
    "\n",
    "Let's revisit `CountVectorizer` and see what kind of vocabulary we are dealing with in the music reviews \"body_without_digits\" column. Whoa, that is a lot of words!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countvec = CountVectorizer()\n",
    "sparse_dtm = countvec.fit_transform(reviews['body_without_digits'])\n",
    "print(sparse_dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This format is actually called Compressed Sparse Format and is useful because we can save huge document term matrices in this format - but it is difficult to look at for a human. Let's convert it to a format we are more familiar with - a data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm = pd.DataFrame(sparse_dtm.toarray(), columns=countvec.get_feature_names(), index=reviews.index)\n",
    "print(dtm.shape)\n",
    "\n",
    "# Whaaaaaaaaaaaat is going on?\n",
    "dtm.head(n = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at just the first row in its entirety\n",
    "dtm.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How about....\n",
    "# Now do a command + f / control + f search for the number 1\n",
    "pd.set_option('display.max_rows', None)\n",
    "dtm.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What can we do with a DTM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Quickly identify the most frequent words:\n",
    "dtm.sum().sort_values(ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the most infrequent words:\n",
    "dtm.sum().sort_values().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the average number of times each word is used in a review:\n",
    "dtm.mean().sort_values(ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF scores\n",
    "\n",
    "How to find distinctive words in a corpus is a long-standing question in text analysis. Today, we'll learn one simple approach to this: TF-IDF. The idea behind words scores is to weight words not just by their frequency, but by their frequency in one document compared to their distribution across all documents. Words that are frequent, but are also used in every single document, will not be distinguishing. We want to identify words that are unevenly distributed across the corpus using TF-IDF. By offsetting the frequency of a word by its document frequency (the number of documents in which it appears) will in theory filter out common terms such as 'the', 'of', and 'and'.\n",
    "\n",
    "Traditionally, the inverse document frequency is calculated as such:\n",
    "\n",
    "**number_of_documents / number_documents_with_term**\n",
    "\n",
    "so:\n",
    "\n",
    "**tfidf_word1 = word1_frequency_document1 * (number_of_documents / number_document_with_word1)**\n",
    "\n",
    "You can, and often should, normalize the numerator: \n",
    "\n",
    "**tfidf_word1 = (word1_frequency_document1 / word_count_document1) * (number_of_documents / number_document_with_word1)**\n",
    "\n",
    "We can calculate this manually, but scikit-learn has a built-in function to do so. This function also uses log frequencies, so the numbers will not correspond excactly to the calculations above. We'll use the [scikit-learn calculation](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDFVectorizer Function\n",
    "\n",
    "To do so, we simply do the same thing we did before with `CountVectorizer`, but instead we use the function `TfidfVectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfvec = TfidfVectorizer()\n",
    "sparse_tfidf = tfidfvec.fit_transform(reviews['body_without_digits'])\n",
    "print(sparse_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn this into a Pandas DataFrame: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = pd.DataFrame(sparse_tfidf.toarray(), columns=tfidfvec.get_feature_names(), index=reviews.index)\n",
    "tfidf.head(n = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the 20 words with highest tf-idf weights:\n",
    "tfidf.max().sort_values(ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok! We have successfully identified content words, without removing stop words. What else do you notice about this list?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying Distinctive Words\n",
    "\n",
    "What can we do with this? These scores are best used when you want to identify distinctive words for individual documents, or groups of documents, compared to other groups or the corpus as a whole. To illustrate this, let's compare three genres and identify the most distinctive words by genre.\n",
    "\n",
    "First we add in a column of genre: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf['genre_'] = reviews['genre']\n",
    "tfidf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets compare the words with the highest tf-idf weight for each genre: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rap = tfidf[tfidf['genre_'] == 'Rap']\n",
    "indie = tfidf[tfidf['genre_'] == 'Indie']\n",
    "jazz = tfidf[tfidf['genre_'] == 'Jazz']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rap.max(numeric_only=True).sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indie.max(numeric_only=True).sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jazz.max(numeric_only=True).sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In week 5 you will learn about topic modeling to see how machines can identify potentially abstract topics in text(s)! [Check out the sweet animation on Wikipedia page](https://en.wikipedia.org/wiki/Topic_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
