{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gensim\n",
    "# !pip install wordcloud\n",
    "# !pip install pyldavis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter # Count most common words\n",
    "import gensim # word2vec model\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import nltk # natural language toolkit\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import movie_reviews # another dataset \n",
    "import numpy as np \n",
    "import os\n",
    "import pandas as pd\n",
    "import pyLDAvis.sklearn # visualize our topic models!\n",
    "import re # regular expressions\n",
    "import seaborn as sns\n",
    "# CV (multiple train/test splitting)\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "# Algorithms (unsupervised)\n",
    "from sklearn.decomposition import LatentDirichletAllocation, PCA\n",
    "# (supervised)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# Tools to create our DTMs\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "# Speed up your machine learning setup\n",
    "from sklearn.pipeline import Pipeline\n",
    "# Mix up our training and test sets\n",
    "from sklearn.utils import shuffle\n",
    "# Super awesome NLP library\n",
    "import spacy\n",
    "# Visualize word clouds \n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category = DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning objectives\n",
    "\n",
    "1. Fit an LDA topic model and visualize it\n",
    "2. Fit a word2vec model and visualize it\n",
    "3. Build a classifier\n",
    "4. Learn a little about BERT!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modeling\n",
    "\n",
    "There are many topic modeling algorithms, but we'll start with [Latent Dirichlet Allocation (LDA)](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation). This is a standard **unsupervised** machine learning text-mining tool that can be used to discover abstract \"topics\" contained within texts.\n",
    "\n",
    "Like the rest of this class, the goal is not to learn everything you need to know about topic modeling. Instead, this will provide you some starter code to run some simple models with the idea that you can use this base of knowledge to explore further. Use the sklearn help files, Stack Overflow, and Google searching to review and learn more about what the code is doing and how to go further. \n",
    "\n",
    "Can you make this code work for your own data? Can you tweak the parameters to get better output?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a dataframe from individual text files\n",
    "\n",
    "You've gathered a bunch of text files, so now what? It is useful to get these files into a dataframe. Python does not make this terribly easy for the beginner, so use the boilerplate code below to help you.\n",
    "\n",
    "Let's concatenate the eleven text files in the \"Data/human-rights/\" folder into a dataframe so we can manipulate that text like we have seen in the previous few notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where am I?\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a variable with the file path for the directory containing the text files\n",
    "# Go two directories up (../../) \n",
    "# and into the Data directory\n",
    "# then into the human-rights subdirectory\n",
    "dir_path = os.listdir(\"../../Data/human-rights/\")\n",
    "\n",
    "# View the contents of this directory\n",
    "dir_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Designate an empty dictionary to store the filename and text as columns\n",
    "for_dataframe = {}\n",
    "\n",
    "# Loop through the directory of text files and open and read them\n",
    "for file in dir_path:\n",
    "    with open(\"../../Data/human-rights/\" + file, \"r\", encoding=\"utf-8\") as to_open:\n",
    "         for_dataframe[file] = to_open.read()\n",
    "            \n",
    "# Create and append the dataframe with two columns - the file name and the text itself\n",
    "human_rights = (pd.DataFrame.from_dict(for_dataframe, \n",
    "                                       orient = \"index\")\n",
    "                .reset_index().rename(index = str, \n",
    "                                      columns = {\"index\": \"File\", 0: \"Text\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_rights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review - manipulate and explore text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out text of one row to make sure it looks okay...\n",
    "list(human_rights[0:1][\"Text\"])\n",
    "# human_rights[0:1][\"Text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic preprocessing\n",
    "\n",
    "Preprocess the text! What else might you want to do that is not included here? Lemmatization? Spacy stuff? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation\n",
    "human_rights[\"Text_processed\"] = human_rights[\"Text\"].map(lambda x: re.sub('[,\\.!?]', '', x))\n",
    "\n",
    "# Convert to lowercase\n",
    "human_rights[\"Text_processed\"] = human_rights[\"Text_processed\"].map(lambda x: x.lower())\n",
    "\n",
    "# Remove digits\n",
    "human_rights['Text_processed'] = human_rights['Text_processed'].str.replace('\\d+', '')\n",
    "\n",
    "# Punctuation and digits are gone! ... ?\n",
    "list(human_rights[0:1][\"Text_processed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_rights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the \"Text_processed\" column as one long string\n",
    "long_string = ','.join(list(human_rights[\"Text_processed\"].values))\n",
    "long_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A few more preprocessing steps..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize long_string\n",
    "hr_tokens = long_string.split()\n",
    "\n",
    "# Remove stopwords\n",
    "stop = stopwords.words(\"english\")\n",
    "no_stops = [word for word in hr_tokens if word not in stopwords.words('english')]\n",
    "freq_hr = Counter(no_stops)\n",
    "\n",
    "# Print the 20 most common words\n",
    "hr_df = pd.DataFrame(freq_hr.most_common(20), columns = [\"Word\", \"Frequency\"])\n",
    "hr_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a BOW model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an empty bag (of words)\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Use the .fit method to tokenize the text and learn the vocabulary\n",
    "vectorizer.fit(human_rights[\"Text_processed\"])\n",
    "\n",
    "# Print the vocabulary\n",
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the DTM\n",
    "\n",
    "Recall that a [document term matrix](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) displays term frequencies or TFIDF scores that occur across a collection of documents. We want to encode the documents into a [sparse matrix](https://sebastianraschka.com/faq/docs/bag-of-words-sparsity.html#:~:text=By%20definition%2C%20a%20sparse%20matrix,as%20a%20word%2Dcount%20vector.&text=Thus%2C%20if%20most%20of%20your,most%20likely%20sparse%20as%20well!) to represent the frequencies or TFIDF scores of each vocabulary word across the documents.\n",
    "\n",
    "Again, the column headers could read **(document number, term)   frequency**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the documents\n",
    "vector = vectorizer.transform(human_rights[\"Text_processed\"])\n",
    "print(vector) #\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "print(vector.shape)\n",
    "print(type(vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View as a multidimensional array before converting to data frame\n",
    "# Rows are the documents\n",
    "# Columns are the terms\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the terms?\n",
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a bigram bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What other processing steps could you include here\n",
    "# ... instead of doing them manually above? \n",
    "bigram_vectorizer = CountVectorizer(ngram_range = (1,2),\n",
    "                                    stop_words = \"english\",\n",
    "                                    token_pattern = r'\\b\\w+\\b', \n",
    "                                    min_df = 1)\n",
    "bigram_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze long_string in the bigram bag of words\n",
    "analyze = bigram_vectorizer.build_analyzer()\n",
    "vocab = analyze(long_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the 20 most commons\n",
    "freq = Counter(vocab)\n",
    "stop_df = pd.DataFrame(freq.most_common(20), columns = [\"Word\", \"Frequency\"])\n",
    "stop_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a word cloud variable\n",
    "cloud = WordCloud(background_color = \"black\", \n",
    "                  max_words = 20, \n",
    "                  contour_width = 5, \n",
    "                  width = 600, height = 300, \n",
    "                  random_state = 5)\n",
    "\n",
    "# Process the word cloud\n",
    "cloud.generate(long_string)\n",
    "\n",
    "# Visualize!\n",
    "cloud.to_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn about using [custom colors here](https://amueller.github.io/word_cloud/auto_examples/a_new_hope.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hr_barplot = sns.barplot(x = \"Frequency\", \n",
    "                         y = \"Word\", \n",
    "                         data = stop_df, \n",
    "                         orient = \"h\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finally! Fit the topic model\n",
    "\n",
    "The input to LDA should be a DTM!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many topics?\n",
    "n_topics = 5\n",
    "# n_topics = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TfidfVectorizer to create the DTM\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df = 0.90,\n",
    "                                   max_features = 500,\n",
    "                                   stop_words = \"english\")\n",
    "\n",
    "# Fit\n",
    "tfidf = tfidf_vectorizer.fit_transform(hr_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Check out this question](https://stackoverflow.com/questions/27697766/understanding-min-df-and-max-df-in-scikit-countvectorizer) to learn more about the `max_df` and `min_df` arguments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate our LDA model\n",
    "lda = LatentDirichletAllocation(n_components = n_topics, \n",
    "                                max_iter = 20, \n",
    "                                random_state = 5)\n",
    "lda = lda.fit(tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a function to print out the top words for each topic in a pretty way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"\\nTopic #{}:\".format(topic_idx))\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the topics\n",
    "tf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "panel = pyLDAvis.sklearn.prepare(lda_model = lda, \n",
    "                                 dtm = tfidf, \n",
    "                                 vectorizer = tfidf_vectorizer, \n",
    "                                 mds = \"tsne\")\n",
    "pyLDAvis.display(panel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 1\n",
    "\n",
    "1. What is a topic? \n",
    "2. What is a \"salient\" term?\n",
    "3. What is the relevance metric lambda?\n",
    "\n",
    "4. What do you know about the eleven human rights documents we used to do this exercise? \n",
    "5. Why are all these topics similar in size in the left plot?\n",
    "6. Why are the overall term frequencies (blue bar) and estimated term frequency, within the selected topic (red bar) similar in the right plot? \n",
    "7. Plug in your own data! You might see more distinct topics given the nature of these human rights documents. Why? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 2\n",
    "\n",
    "Read up on LDA and its visualizations by clicking the below links:\n",
    "- https://www.objectorientedsubject.net/2018/08/experiments-on-topic-modeling-pyldavis/\n",
    "- http://www.cs.columbia.edu/~blei/papers/ChaneyBlei2012.pdf\n",
    "- https://shravan-kuchkula.github.io/topic-modeling/#lda-results\n",
    "- https://markroxor.github.io/gensim/static/notebooks/gensim_news_classification.html\n",
    "- http://vis.stanford.edu/files/2012-Termite-AVI.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec\n",
    "\n",
    "The word2vec family of algorithms use shallow neural networks to produce word embeddings, or ways to represent similar words similarly as numbers. We will explore neural network architecture in notebook 5-2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# First, store the documents we want to explore in a separate dataframe with just one column\n",
    "w2v_df = pd.DataFrame({'Processed': human_rights[\"Text_processed\"]})\n",
    "w2v_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn the text of each row into a list\n",
    "# We now have a list of lists - one for each document\n",
    "split_rows = [row.split() for row in w2v_df['Processed']]\n",
    "split_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the word2vec model\n",
    "model = gensim.models.Word2Vec(split_rows, \n",
    "                               min_count = 2,\n",
    "                               size = 12, \n",
    "                               workers = 3, \n",
    "                               window = 3, \n",
    "                               sg = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the vocabulary \n",
    "words = list(model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model[\"human\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare! \n",
    "model.similarity(\"the\", \"the\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.similarity(\"human\", \"rights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.similarity(\"human\", \"the\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.similarity(\"human\", \"law\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.similarity(\"country\", \"law\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.similarity(\"justice\", \"law\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.similarity(\"international\", \"law\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.similarity(\"united\", \"nations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjectives\n",
    "model.wv.most_similar(positive = \"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = model.wv.most_similar(positive = \"rights\", topn=10)\n",
    "print(type(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = DataFrame(your_list,columns=['Column_Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar(positive = \"war\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.similarity(\"peace\", \"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.similarity(\"war\", \"human\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot words with PCA\n",
    "\n",
    "[Principal component analysis](https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60) and related dimension-reduction algorithms are an excellent way to visualize multivariate data in reduced dimensional space - such as a 2D scatterplot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the word2vec vocab\n",
    "features = model[model.wv.vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters of our PCA\n",
    "\n",
    "# Just look at the first two dimensions - the X and Y axes\n",
    "for_pca = PCA(n_components = 2)\n",
    "pca_out = for_pca.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot!\n",
    "plt.scatter(pca_out[:, 0], pca_out[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotate text labels\n",
    "\n",
    "What if we want to lable points with just certain words? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8,6))\n",
    "plt.scatter(pca_out[:, 0], pca_out[:, 1])\n",
    "words = list(model.wv.vocab)\n",
    "# Annotate only the top 20 words [0:20]\n",
    "for i, word in enumerate(words[0:20]):\n",
    "    plt.annotate(word, size = 20, xy = (pca_out[i, 0], pca_out[i, 1]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or, just the top 20 words?\n",
    "plt.figure(figsize = (8,6))\n",
    "plt.scatter(pca_out[:, 0][0:20], pca_out[:, 1][0:20])\n",
    "words = list(model.wv.vocab)\n",
    "for i, word in enumerate(words[0:20]):\n",
    "    plt.annotate(word, size = 20, xy=(pca_out[i, 0], pca_out[i, 1]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://kavita-ganesan.com/gensim-word2vec-tutorial-starter-code/#.XuxYm2pKjOQ\n",
    "\n",
    "https://machinelearningmastery.com/develop-word-embeddings-python-gensim/\n",
    "\n",
    "https://www.kaggle.com/pierremegret/gensim-word2vec-tutorial\n",
    "\n",
    "https://towardsdatascience.com/a-beginners-guide-to-word-embedding-with-gensim-word2vec-model-5970fa56cc92\n",
    "\n",
    "https://www.datacamp.com/community/blog/spacy-cheatsheet\n",
    "\n",
    "https://code.google.com/archive/p/word2vec/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification of Textual Data\n",
    "\n",
    "How can we translate this simple model of binary classification to text? Let's look at a corpus from `nltk` and build your own classifier using sklearn's machine learning `Pipeline`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the nltk built-in movie reviews dataset\n",
    "nltk.download(\"movie_reviews\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you might expect, this is a corpus of IMDB movie reviews. Someone went through and read each review, labeling it as either \"positive\" or \"negative\". The task we have before us is to create a model that can accurately predict whether a never-before-seen review is positive or negative. \n",
    "\n",
    "From the `movie_reviews` object let's take out the reviews and the judgement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract our x (reviews) and y (judgements) variables\n",
    "reviews = [movie_reviews.raw(fileid) for fileid in movie_reviews.fileids()]\n",
    "judgements = [movie_reviews.categories(fileid)[0] for fileid in movie_reviews.fileids()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save in a dataframe\n",
    "movies = pd.DataFrame({\"Reviews\" : reviews, \n",
    "                      \"Judgements\" : judgements})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We ahve 2000 movie reviews\n",
    "movies.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a random review and its judgement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The human annotator's review was:\", movies.Judgements[0])\n",
    "print()\n",
    "print(movies.Reviews[0][:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So right now we have a dataframe of movie reviews in the `Reviews` variable and a list of their corresponding judgements in the `Judgements` column. Awesome. What does this sound like to you? Independent and dependent variables? You'd be right!\n",
    "\n",
    "`Reviews` is our x variable. `Judgements` is our y variable. Let's first reassign x and y for simplicity. While we're at it, we're going to set the random_state for our computer. Remember that this makes our result reproducible. We'll also `shuffle` so that we randomize the order of our observations, and when we split the testing and training data it won't be in a biased order. However, start learning about [stratified sampling](https://en.wikipedia.org/wiki/Stratified_sampling) and when you should use it instead of `shuffle`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = shuffle(np.array(movies.Reviews), np.array(movies.Judgements), random_state = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't believe me that all we did is reassign and shuffle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[0], print(\"Human annotator's review was:\", y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get meaningful independent variables (words) we have to do some processing too (think DTM!). With `sklearn`'s text pipelines, we can quickly build a text classifier in only a few lines of Python: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-validated pipepline\n",
    "\n",
    "Remember training/test splitting? Lets do this `cv = 20` times! https://en.wikipedia.org/wiki/Cross-validation_(statistics)\n",
    "\n",
    "We should reasonably expect this to perform better than a single training/test split (see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validated model!\n",
    "text_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1, 2))),\n",
    "                    ('tfidf', TfidfTransformer()),\n",
    "                    ('clf', LogisticRegression(random_state = 0, penalty = \"l2\", C = 1000))\n",
    "                     ])\n",
    "\n",
    "scores = cross_val_score(text_clf, x, y, cv = 20)\n",
    "\n",
    "print(scores, np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Whoa! What just happened?!?*** The pipeline tells us three things happened:\n",
    "\n",
    "1. `CountVectorizer`\n",
    "\n",
    "2. `TfidfTransformer`\n",
    "\n",
    "3. `LogisticRegression`\n",
    "\n",
    "Let's walk through this step by step.\n",
    "\n",
    "1. `CountVectorizer` does the same as before. It changes all the texts to quickly normalized words, and then simply counts the frequency of each word occuring in the corpus for each document. The feature array for each document at this point is simply the length of all unique words in a corpus, with the count for the frequency of each. This is the most basic way to provide features for a classifier - a document term matrix!\n",
    "\n",
    "2. Remember that tfidf (term frequency inverse document frequency) is an algorithm that aims to find words that are important to specific documents. It does this by taking the term frequency (tf) for a specific term in a specific document, and multiplying it by the term's inverse document frequency (idf), which is the total number of documents divided by the number of documents that contain the term at least once. `TfidfTransformer` transforms the `CountVectorizer` into a tf-idf representation. \n",
    "\n",
    "A tfidf value is calculated for each term for each document. The feature arrays for a document is now the tfidf values. \n",
    "\n",
    "> Remember! The tfidf matrix is similar to our document term matrix, only now the values have been weighted according to their distribution across documents.\n",
    "\n",
    "The pipeline now sends these tfidf feature arrays to \n",
    "\n",
    "3. `LogisticRegression`, what we learned from notebook 4-3. We add in an l2 penalization parameter because we have many more independent variables from our `dtm` than observations. \n",
    "\n",
    "Check out the [pipeline documentation here](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)\n",
    "\n",
    "The code below breaks this down by each step, but combines the `CountVectorizer` and `TfidfTransformer` in the `TfidfVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard training/test split (no cross validation)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.30, random_state = 0)\n",
    "\n",
    "# get tfidf values\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf.fit(x)\n",
    "x_train = tfidf.transform(x_train)\n",
    "x_test = tfidf.transform(x_test)\n",
    "\n",
    "# build and test logit\n",
    "logit_class = LogisticRegression(random_state = 0, penalty = \"l2\", C = 1000)\n",
    "model = logit_class.fit(x_train, y_train)\n",
    "model.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important Features\n",
    "\n",
    "After we train the model we can then index the tfidf matrix for the words with the most significant coefficients (remember independent variables!) to get the most helpful features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = tfidf.get_feature_names()\n",
    "top25pos = np.argsort(model.coef_[0])[-25:]\n",
    "print(\"Top features for positive reviews:\")\n",
    "print(list(feature_names[j] for j in top25pos))\n",
    "print()\n",
    "print(\"Top features for negative reviews:\")\n",
    "top25neg = np.argsort(model.coef_[0])[:25]\n",
    "print(list(feature_names[j] for j in top25neg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction\n",
    "\n",
    "We can also use our model to classify new reviews, all we have to do is extract the tfidf features from the raw text and send them to the model as our features (independent variables):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_bad_review = \"This was the most awful worst super bad movie ever!\"\n",
    "\n",
    "features = tfidf.transform([new_bad_review])\n",
    "\n",
    "model.predict(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_good_review = \"This movie was great, awesome and good!\"\n",
    "\n",
    "features = tfidf.transform([new_good_review])\n",
    "\n",
    "model.predict(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bidirectional Encoder Representations from Transformers (BERT)\n",
    "\n",
    "Want to go really crazy? The [BERT algorithmic family](https://www.blog.google/products/search/search-language-understanding-bert/) is the way to go!\n",
    "\n",
    "https://github.com/google-research/bert  \n",
    "https://github.com/google-research/bert#pre-trained-models  \n",
    "http://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
